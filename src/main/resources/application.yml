
server:
  port: 8032
  servlet:
    context-path: /todo

# single file max size
spring:
  http:
    multipart:
      maxFileSize: 30Mb
      maxRequestSize: 100Mb    # total size per request
  datasource:
    url: jdbc:mysql://localhost:3306/manage?useUnicode=true&characterEncoding=utf8&serverTimezone=Hongkong
    username: root
    password: opening2000
    driver-class-name: com.mysql.cj.jdbc.Driver
    max-active: 20
    max-idle: 8
    min-idle: 8
    initial-size: 10
  thymeleaf:
    encoding: UTF-8
    content-type: text/html
    cache: false
    mode: HTML
  resource:
    chain:
      strategy:
        content:
          enabled: true
          paths: /**
  mvc:
    view:
      prefix: classpath:/templates/
      suffix: .html
  jackson:
    date-format: yyyy-MM-dd HH:mm:ss
    time-zone: Asia/Shanghai
  kafka:
    bootstrap-servers: 192.168.19.129:9092
    producer:
      retries: 0  # 重试次数
      acks: 1     # 应答级别:多少个分区副本备份完成时向生产者发送ack确认(可选0、1、all/-1)
      batch-size: 16384    # 批量大小
      properties:
        linger:
          ms: 0   # 提交延时
      buffer-memory: 33554432    # 生产端缓冲区大小
      key-serializer: org.apache.kafka.common.serialization.StringSerializer    # Kafka提供的序列化和反序列化类
      value-serializer: org.apache.kafka.common.serialization.StringSerializer      
    consumer:
      properties:
        group:
          id: task
        session:
          timeout:
            ms: 120000            # 消费会话超时时间(超过这个时间consumer没有发送心跳,就会触发rebalance操作)
        request:
          timeout:
            ms: 180000            # 消费请求超时时间
      enable-auto-commit: true    # 是否自动提交offset
      auto:
        commit:
          interval:
            ms: 1000
      key-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      value-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      #max-poll-records: 50    # 批量消费
    listener:
      missing-topics-fatal: false  # 消费端监听的topic不存在时，项目启动会报错(关掉)
      #type: batch    # 设置批量消费
    message:
      consumer:
        bootstrap-servers: 192.168.19.129:9092
        topics: task_message_topic
        group-id: message01          # 默认消费者组,业务标识,数据库不同时需要避免使用同一个
        auto-offset-reset: earliest  # latest（从最新的消息开始消费）,earliest（从最老的消息开始消费）,none（如果无offset就抛出异常）
        fetch-min-size: 1            # 读取消息时最小的Payload的字节数
        max-poll-records: 2500       # consumer每次批量拉多少条数据。
        enable-auto-commit: false    # 是否自动提交消费位移
        auto-commit-interval: 2000
        value-deserializer: org.apache.kafka.common.serialization.StringDeserializer
        key-deserializer: org.apache.kafka.common.serialization.StringDeserializer

# mybatis
mybatis:
  mapper-locations: classpath:mapper/*.xml

kafka:
  task:
    topic: task_topic
